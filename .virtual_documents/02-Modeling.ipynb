# Import relevant libraries and packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sqlite3
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import lightgbm as lgb
from sklearn.metrics import mean_absolute_error, r2_score
import warnings


# --- 1. Load Data ---
DB_PATH = 'data/cta_database.db'
conn = sqlite3.connect(DB_PATH)
query = "SELECT * FROM train_positions"
df = pd.read_sql_query(query, conn)
conn.close()


# Re-create the datetime features from EDA
df['fetch_datetime'] = pd.to_datetime(df['fetch_timestamp'], unit='s')
df['hour_of_day'] = df['fetch_datetime'].dt.hour
df['day_of_week'] = df['fetch_datetime'].dt.day_name()


# --- 2. Define Target Variable and Features ---
# For this first model, let's predict speed.
# We'll drop rows with missing data for simplicity.
df.dropna(subset=['latitude', 'longitude', 'heading'], inplace=True)


TARGET = 'speed_kmh' # We need to calculate this feature first!


# We'll calculate speed just like in the EDA notebook
# (This ensures our modeling notebook is self-contained)
def haversine_distance(lat1, lon1, lat2, lon2):
    R = 6371000
    phi1 = np.radians(lat1)
    phi2 = np.radians(lat2)
    delta_phi = np.radians(lat2 - lat1)
    delta_lambda = np.radians(lon2 - lon1)
    a = np.sin(delta_phi / 2.0)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2.0)**2
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))
    return R * c


df_sorted = df.sort_values(by=['run_number', 'fetch_timestamp']).copy()
df_sorted['time_diff_s'] = df_sorted.groupby('run_number')['fetch_timestamp'].diff()
df_sorted['lat_prev'] = df_sorted.groupby('run_number')['latitude'].shift(1)
df_sorted['lon_prev'] = df_sorted.groupby('run_number')['longitude'].shift(1)
df_sorted['distance_m'] = haversine_distance(df_sorted['lat_prev'], df_sorted['lon_prev'], df_sorted['latitude'], df_sorted['longitude'])
df_sorted['speed_mps'] = (df_sorted['distance_m'] / df_sorted['time_diff_s']).fillna(0)
df_sorted['speed_kmh'] = df_sorted['speed_mps'] * 3.6
df = df_sorted.dropna(subset=[TARGET]).copy()


# Define which columns are features (X) and which is the target (y)
features = ['latitude', 'longitude', 'heading', 'hour_of_day', 'day_of_week', 'is_delayed', 'next_station_name']
X = df[features]
y = df[TARGET]


# --- 3. Time-Based Data Split ---
# It's crucial to split time-series data chronologically
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, shuffle=False)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)
print(f"Training set size: {len(X_train)}")
print(f"Validation set size: {len(X_val)}")
print(f"Test set size: {len(X_test)}")


# --- 4. Preprocessing Pipeline Setup ---
# Define which columns need which transformation
numerical_features = ['latitude', 'longitude', 'heading', 'hour_of_day']
categorical_features = ['day_of_week', 'next_station_name', 'is_delayed']


# Create the preprocessing pipelines for numerical and categorical data
numerical_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown='ignore')


# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='passthrough' # Keep other columns (if any)
)


# Display the first few rows of the training data
display(X_train.head())


# --- 5. Create and Train the LightGBM Model ---

# Create a full pipeline that includes the preprocessor and the model
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', lgb.LGBMRegressor(random_state=42))
])


# Train the entire pipeline on the training data
print("Training the LightGBM model...")
pipeline.fit(X_train, y_train)
print("Training complete.")


# --- 6. Evaluate the Model ---

# Make predictions on the validation set
y_pred_val = pipeline.predict(X_val)


# Calculate performance metrics
mae = mean_absolute_error(y_val, y_pred_val)
r2 = r2_score(y_val, y_pred_val)
print(f"\n--- Model Performance on Validation Set ---")
print(f"Mean Absolute Error (MAE): {mae:.2f} km/h")
print(f"R-squared (RÂ²): {r2:.2f}")
print("-----------------------------------------")


# Display a few sample predictions vs actual values
results_df = pd.DataFrame({'Actual Speed': y_val, 'Predicted Speed': y_pred_val})
print("\nSample Predictions:")
display(results_df.head(10))



