# Import relevant librarie and packages
import numpy as np
import pandas as pd
import sqlite3
import matplotlib.pyplot as plt
import seaborn as sns
import geopandas as gpd
from shapely.geometry import Point


plt.rcParams.update({
    "pgf.texsystem": "pdflatex",
    "font.family": "serif",
    "text.usetex": True,
    "pgf.rcfonts": False,
})


# Set some display options for pandas
pd.set_option('display.max_columns', 50)


# Path to the database
DB_PATH = "data/cta_database.db"


# Connect to SQLite database
conn = sqlite3.connect(DB_PATH)


# SQL query to select all data from the table
query = "SELECT * FROM train_positions"


# Load the data into a Pandas DataFrame
df = pd.read_sql_query(query, conn)


# Close the connection
conn.close()


# --- Initial Inspection ---
# Print the shape of the DataFrame
print(f"Shape of the dataset: {df.shape}")


# Print the first 5 rows to see what the data looks like
print("\nFirst 5 rows:")
df.head()


# Print a concise summary of the DataFrame
print("\nData Info:")
df.info()


# --- Create the GeoDataFrame from our train data ---
# First, drop any rows that might have missing lat/lon data
df_geo = df.dropna(subset=['latitude', 'longitude']).copy()


# Create a 'geometry' column from the latitude and longitude
# This is what makes the DataFrame "map-aware"
geometry = [Point(xy) for xy in zip(df_geo['longitude'], df_geo['latitude'])]
gdf_trains = gpd.GeoDataFrame(df_geo, geometry=geometry, crs="EPSG:4326")


# --- Load the Chicago map shapefile ---
chicago_map = gpd.read_file("geo_data/Boundaries_-_City_20250823.geojson")


# --- Create the Plot ---
fig, ax = plt.subplots(figsize=(12, 12))

# Plot the map of Chicago as the base layer
chicago_map.plot(ax=ax, color='lightgray', edgecolor='black')

# --- FIX: Adjust the sample size dynamically ---
# Set the sample size to 1000 or the total number of rows, whichever is smaller
sample_size = min(1000, len(gdf_trains))

# Plot the train positions on top using the adjusted sample size
if sample_size > 0:
    gdf_trains.sample(n=sample_size, random_state=42).plot(
        ax=ax, marker='o', color='red', markersize=5, alpha=0.5, label='Train Positions'
    )

# Customize the plot
ax.set_title('CTA Red Line Train Positions in Chicago')
ax.set_xlabel('Longitude')
ax.set_ylabel('Latitude')
ax.legend()
plt.show()


# Convert Unix timestamps to datetime objects
df['fetch_datetime'] = pd.to_datetime(df['fetch_timestamp'], unit='s')
df['arrival_datetime'] = pd.to_datetime(df['arrival_time'], unit='s')


# Extract useful time features like hour of the day and day of the week
df['hour_of_day'] = df['fetch_datetime'].dt.hour
df['day_of_week'] = df['fetch_datetime'].dt.day_name()


# Display the new columns
display(df[['fetch_datetime', 'arrival_datetime', 'hour_of_day', 'day_of_week']].head())


# Plot the number of active trains per hour
plt.figure(figsize=(12, 6))
sns.countplot(data=df, x='hour_of_day', palette='viridis')
plt.title('Number of Active Trains by Hour of Day')
plt.xlabel('Hour of Day (24-hour format)')
plt.ylabel('Number of Data Points (Count of Active Trains)')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()


def haversine_distance(lat1, lon1, lat2, lon2):
    """Calculates the distance between two points on Earth in meters."""
    R = 6371000  # Radius of Earth in meters
    phi1 = np.radians(lat1)
    phi2 = np.radians(lat2)
    delta_phi = np.radians(lat2 - lat1)
    delta_lambda = np.radians(lon2 - lon1)
    
    a = np.sin(delta_phi / 2.0)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2.0)**2
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))
    
    return R * c


# Sort the data so consecutive readings for each train are next to each other
df_sorted = df.sort_values(by=['run_number', 'fetch_timestamp']).copy()


# Group by train and calculate the change in time and position
df_sorted['time_diff_s'] = df_sorted.groupby('run_number')['fetch_timestamp'].diff()
df_sorted['lat_prev'] = df_sorted.groupby('run_number')['latitude'].shift(1)
df_sorted['lon_prev'] = df_sorted.groupby('run_number')['longitude'].shift(1)


# Calculate the distance traveled between readings
df_sorted['distance_m'] = haversine_distance(
    df_sorted['lat_prev'], df_sorted['lon_prev'],
    df_sorted['latitude'], df_sorted['longitude']
)


# Calculate speed in meters per second (and handle cases where time_diff is zero)
df_sorted['speed_mps'] = (df_sorted['distance_m'] / df_sorted['time_diff_s']).fillna(0)


# Convert to a more intuitive unit like kilometers per hour
df_sorted['speed_kmh'] = df_sorted['speed_mps'] * 3.6


# --- Visualize Speed Distribution ---
plt.figure(figsize=(12, 6))
# We'll filter out speeds over 100 km/h as they are likely errors from GPS jumps
sns.histplot(df_sorted[df_sorted['speed_kmh'] < 100]['speed_kmh'], bins=50, kde=True)
plt.title('Distribution of Calculated Train Speeds')
plt.xlabel('Speed (km/h)')
plt.ylabel('Frequency')
plt.show()


# Calculate the percentage of readings that are flagged as delayed
delay_percentage = df['is_delayed'].value_counts(normalize=True) * 100
print(f"Percentage of delayed readings:\n{delay_percentage}\n")


# Visualize delays by hour of the day (updated code)
plt.figure(figsize=(12, 10))

# Use errorbar=None instead of ci=None
# Assign 'hour_of_day' to hue to maintain the color palette
sns.barplot(
    data=df, 
    x='hour_of_day', 
    y='is_delayed', 
    errorbar=None,  # Updated parameter
    palette='coolwarm',
    hue='hour_of_day', # Added to maintain color mapping
    legend=False     # Added to hide the unnecessary legend
)

plt.title('Proportion of Delayed Trains by Hour of Day')
plt.xlabel('Hour of Day')
plt.ylabel('Proportion of Readings Flagged as "Delayed"')
plt.show()


# Assign the feature-engineered dataframe back to your main df
df = df_sorted


# Filter out speeds that are likely errors or when the train is stopped
df_speed = df[(df['speed_kmh'] > 1) & (df['speed_kmh'] < 100)].copy()


# Calculate the mean and standard deviation of speed for each hour of the day
hourly_stats = df_speed.groupby('hour_of_day')['speed_kmh'].agg(['mean', 'std']).reset_index()
hourly_stats.rename(columns={'mean': 'hourly_mean_speed', 'std': 'hourly_std_speed'}, inplace=True)


# Merge these hourly stats back into our main dataframe
df_speed = pd.merge(df_speed, hourly_stats, on='hour_of_day', how='left')


# Calculate the Z-score for each data point
df_speed['speed_zscore'] = (df_speed['speed_kmh'] - df_speed['hourly_mean_speed']) / df_speed['hourly_std_speed']


# --- Identify and Display Anomalies ---
z_score_threshold = 3
df_speed['is_anomaly'] = df_speed['speed_zscore'].abs() > z_score_threshold

anomalies = df_speed[df_speed['is_anomaly']]

print(f"\nFound {len(anomalies)} potential point anomalies (Z-score > {z_score_threshold}).")


# Display the top 10 most anomalous readings
print("\nTop 10 Anomalous Readings:")
display(anomalies.sort_values(by='speed_zscore', ascending=False).head(10))



